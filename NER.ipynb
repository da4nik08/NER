{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62e5dd4-adc6-4ac4-b58e-27b0f398f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import custom_dataset\n",
    "import pickle\n",
    "import tiktoken\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from custom_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a1aea-9f69-4e24-84c0-15c82a946353",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pickle Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096e05d2-7ef5-4651-a0b8-c65fc55d4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(file_path, data):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e955856-9906-45bc-9e67-da3e4e2acc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pkl(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e2b0d-75ec-4597-a095-b605cd254064",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12834357-21c1-406d-91f8-e335f1d831d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_pkl(\"dataset/x.pkl\")\n",
    "Y = get_pkl(\"dataset/y.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382910fa-bf91-483a-baa8-be78b9a6c16f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Encode tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b833e6b-dac0-450a-8ae4-188848cd4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cad7576-6c27-4317-bf5b-9d9ab05e6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(result, enc2):\n",
    "    output = list()\n",
    "    for i in result:\n",
    "        output.append(enc2.encode(i, allowed_special={'<|im_start|>', \"<|im_end|>\"}))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc22f254-6eb6-4d89-b741-d9d6ca870a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "enc2 = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|im_start|>\": 100264,\n",
    "        \"<|im_end|>\": 100265,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66026c9-6038-4578-9832-06345c67363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = [\"<|im_start|>\" + sentence + \"<|im_end|>\" for sentence in X]\n",
    "new_Y = [\"<|im_start|>\" + sentence + \"<|im_end|>\" for sentence in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe035ff-20e6-470a-9e19-2452d9e2e7bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "enc_X = tokenization(X, enc)\n",
    "enc_Y = tokenization(Y, enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95cdaed5-2aa6-4e01-9d11-99be3e1b913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_mount(sentence):\n",
    "    if 'mount' in sentence:\n",
    "        return sentence.replace('mount ', '')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19528498-c3f8-4575-9602-cc7dc82e747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Y = [delete_mount(sentence) for sentence in new_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64a61ab-1570-4120-9369-0fe1b3359313",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_X2 = tokenization(new_X, enc2)\n",
    "enc_Y2 = tokenization(new_Y, enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da52993-7fc0-44e2-ac1f-e3a3cd07bf3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>mckinley<|im_end|>',\n",
       " '<|im_start|>aconcagua<|im_end|>',\n",
       " '<|im_start|>aconcagua<|im_end|>',\n",
       " '<|im_start|>machu picchu<|im_end|>',\n",
       " '<|im_start|>damavand<|im_end|>',\n",
       " '<|im_start|>denali<|im_end|>',\n",
       " '<|im_start|>rainier<|im_end|>',\n",
       " '<|im_start|>annapurna<|im_end|>',\n",
       " '<|im_start|>whitney<|im_end|>',\n",
       " '<|im_start|>pikes peak, rainier<|im_end|>',\n",
       " '<|im_start|>gissar range<|im_end|>',\n",
       " '<|im_start|>blackburn<|im_end|>',\n",
       " '<|im_start|>k2<|im_end|>',\n",
       " '<|im_start|>kosciuszko<|im_end|>',\n",
       " '<|im_start|>aconcagua<|im_end|>',\n",
       " '<|im_start|>saint helena<|im_end|>',\n",
       " '<|im_start|>cook<|im_end|>',\n",
       " '<|im_start|>mckinley<|im_end|>',\n",
       " '<|im_start|>k2<|im_end|>',\n",
       " '<|im_start|>baker<|im_end|>',\n",
       " '<|im_start|>everest<|im_end|>',\n",
       " '<|im_start|>olympus<|im_end|>',\n",
       " '<|im_start|>kilimanjaro<|im_end|>',\n",
       " '<|im_start|>marcus baker<|im_end|>',\n",
       " '<|im_start|>huascarã¡n<|im_end|>',\n",
       " '<|im_start|>bogd khan uul<|im_end|>',\n",
       " '<|im_start|>vesuvius<|im_end|>',\n",
       " '<|im_start|>elbrus<|im_end|>',\n",
       " '<|im_start|>kilimanjaro<|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|>kebnekaise<|im_end|>',\n",
       " '<|im_start|>fuji<|im_end|>',\n",
       " '<|im_start|>everest<|im_end|>',\n",
       " '<|im_start|>nanga parbat<|im_end|>',\n",
       " '<|im_start|>kinabalu<|im_end|>',\n",
       " '<|im_start|>matterhorn<|im_end|>',\n",
       " '<|im_start|>denali<|im_end|>',\n",
       " '<|im_start|>ararat<|im_end|>',\n",
       " '<|im_start|>aconcagua<|im_end|>',\n",
       " '<|im_start|>isto<|im_end|>',\n",
       " '<|im_start|>gerlachovsky stit<|im_end|>',\n",
       " '<|im_start|>k2<|im_end|>',\n",
       " '<|im_start|>kangchenjunga<|im_end|>',\n",
       " '<|im_start|>annapurna<|im_end|>',\n",
       " '<|im_start|>apo<|im_end|>',\n",
       " '<|im_start|>matterhorn<|im_end|>',\n",
       " '<|im_start|>ararat<|im_end|>',\n",
       " '<|im_start|>trolltunga<|im_end|>',\n",
       " '<|im_start|>cook<|im_end|>',\n",
       " '<|im_start|>annapurna<|im_end|>',\n",
       " '<|im_start|>kenya<|im_end|>',\n",
       " '<|im_start|>hood<|im_end|>',\n",
       " '<|im_start|>kilimanjaro<|im_end|>',\n",
       " '<|im_start|>mont blanc<|im_end|>',\n",
       " '<|im_start|>blackburn<|im_end|>',\n",
       " '<|im_start|>elbrus<|im_end|>',\n",
       " '<|im_start|>puncak jaya<|im_end|>',\n",
       " '<|im_start|>annapurna<|im_end|>',\n",
       " '<|im_start|>elbrus<|im_end|>',\n",
       " '<|im_start|>mont blanc<|im_end|>',\n",
       " '<|im_start|>tre cime di lavaredo, marmolada<|im_end|>',\n",
       " '<|im_start|>olympus<|im_end|>',\n",
       " '<|im_start|>mitchell<|im_end|>',\n",
       " '<|im_start|>waddington<|im_end|>',\n",
       " '<|im_start|>fuji<|im_end|>',\n",
       " '<|im_start|>triglav<|im_end|>',\n",
       " '<|im_start|>carstensz pyramid<|im_end|>',\n",
       " '<|im_start|>mont blanc<|im_end|>',\n",
       " '<|im_start|>fuji<|im_end|>',\n",
       " '<|im_start|>roraima, kilimanjaro<|im_end|>',\n",
       " '<|im_start|>ama dablam, logan<|im_end|>',\n",
       " '<|im_start|>vinson, blanc<|im_end|>',\n",
       " '<|im_start|>saana, fitz roy<|im_end|>',\n",
       " '<|im_start|>denali, denali<|im_end|>',\n",
       " '<|im_start|>aconcagua, graham<|im_end|>',\n",
       " '<|im_start|>ararat, mont blanc<|im_end|>',\n",
       " '<|im_start|>sonder, khã¼iten peak<|im_end|>',\n",
       " '<|im_start|>el capitan, shishapangma<|im_end|>',\n",
       " '<|im_start|>robson, robson<|im_end|>',\n",
       " '<|im_start|>aconcagua, everest<|im_end|>',\n",
       " '<|im_start|>cho oyu, annapurna<|im_end|>',\n",
       " '<|im_start|>illimani, k2<|im_end|>',\n",
       " '<|im_start|>aneto, monte perdido, humphreys peak<|im_end|>',\n",
       " '<|im_start|>alps, kilimanjaro<|im_end|>',\n",
       " '<|im_start|>aconcagua, aconcagua<|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|>denali, logan<|im_end|>',\n",
       " '<|im_start|>aconcagua, blanc<|im_end|>',\n",
       " '<|im_start|>ararat, fitz roy<|im_end|>',\n",
       " '<|im_start|>sonder, denali<|im_end|>',\n",
       " '<|im_start|>el capitan, graham<|im_end|>',\n",
       " '<|im_start|>robson, mont blanc<|im_end|>',\n",
       " '<|im_start|>aconcagua, khã¼iten peak<|im_end|>',\n",
       " '<|im_start|>cho oyu, shishapangma<|im_end|>',\n",
       " '<|im_start|>illimani, robson<|im_end|>',\n",
       " '<|im_start|>aneto, monte perdido, everest<|im_end|>',\n",
       " '<|im_start|>alps, annapurna<|im_end|>',\n",
       " '<|im_start|>aconcagua, k2<|im_end|>',\n",
       " '<|im_start|>kilimanjaro, humphreys peak<|im_end|>',\n",
       " '<|im_start|>kilimanjaro, ontake<|im_end|>',\n",
       " '<|im_start|>aconcagua, churchill<|im_end|>',\n",
       " '<|im_start|>denali, gasherbrum ii<|im_end|>',\n",
       " '<|im_start|>makalu, fuji<|im_end|>',\n",
       " '<|im_start|>kilimanjaro, mustagh ata<|im_end|>',\n",
       " '<|im_start|>huayna picchu, fuji<|im_end|>',\n",
       " '<|im_start|>rainier, elbrus<|im_end|>',\n",
       " '<|im_start|>rainier, zugspitze<|im_end|>',\n",
       " '<|im_start|>mckinley, kangchenjunga<|im_end|>',\n",
       " '<|im_start|>rainier, swiss alps, matterhorn<|im_end|>',\n",
       " '<|im_start|>everest, matterhorn<|im_end|>',\n",
       " '<|im_start|>everest, k2<|im_end|>',\n",
       " '<|im_start|>steele, cotopaxi<|im_end|>',\n",
       " '<|im_start|>annapurna<|im_end|>',\n",
       " '<|im_start|>hood<|im_end|>',\n",
       " '<|im_start|>aconcagua<|im_end|>',\n",
       " '<|im_start|>hood<|im_end|>',\n",
       " '<|im_start|>damavand<|im_end|>',\n",
       " '<|im_start|>kilimanjaro<|im_end|>',\n",
       " '<|im_start|>ben nevis<|im_end|>',\n",
       " '<|im_start|>k2<|im_end|>',\n",
       " '<|im_start|>kilimanjaro<|im_end|>',\n",
       " '<|im_start|>elbrus<|im_end|>',\n",
       " '<|im_start|>vancouver<|im_end|>',\n",
       " '<|im_start|>ben nevis<|im_end|>',\n",
       " '<|im_start|>olympus<|im_end|>',\n",
       " '<|im_start|>aspiring, saint elias<|im_end|>',\n",
       " '<|im_start|>denali, matterhorn<|im_end|>',\n",
       " '<|im_start|>mckinley, cook<|im_end|>',\n",
       " '<|im_start|>annapurna, shasta<|im_end|>',\n",
       " '<|im_start|>himalayas, sierra nevada<|im_end|>',\n",
       " '<|im_start|>everest, fuji<|im_end|>',\n",
       " '<|im_start|>robson, elbrus<|im_end|>',\n",
       " '<|im_start|>fairweather, hood<|im_end|>',\n",
       " '<|im_start|>damavand, charleston<|im_end|>',\n",
       " '<|im_start|>olympus, vinson<|im_end|>',\n",
       " '<|im_start|>elbrus, everest<|im_end|>',\n",
       " '<|im_start|>uluru, pico da bandeira<|im_end|>',\n",
       " '<|im_start|>vinson massif, mckinley<|im_end|>',\n",
       " '<|im_start|>rainier<|im_end|>',\n",
       " '<|im_start|>kilimanjaro<|im_end|>',\n",
       " '<|im_start|>matterhorn<|im_end|>',\n",
       " '<|im_start|>humphreys peak<|im_end|>',\n",
       " '<|im_start|>fuji<|im_end|>',\n",
       " '<|im_start|>k2<|im_end|>',\n",
       " '<|im_start|>robson<|im_end|>',\n",
       " '<|im_start|>dhaulagiri<|im_end|>',\n",
       " '<|im_start|>k2<|im_end|>',\n",
       " '<|im_start|>bear<|im_end|>',\n",
       " '<|im_start|>jabal haroun<|im_end|>',\n",
       " '<|im_start|>matterhorn<|im_end|>',\n",
       " '<|im_start|>grand teton<|im_end|>',\n",
       " '<|im_start|>atlas<|im_end|>',\n",
       " '<|im_start|>whistler blackcomb<|im_end|>',\n",
       " '<|im_start|>denali<|im_end|>',\n",
       " '<|im_start|>aconcagua<|im_end|>',\n",
       " '<|im_start|>rainier<|im_end|>',\n",
       " '<|im_start|>denali<|im_end|>',\n",
       " '<|im_start|>kilimanjaro<|im_end|>',\n",
       " '<|im_start|>everest<|im_end|>',\n",
       " '<|im_start|>saltoro kangri<|im_end|>',\n",
       " '<|im_start|>rocky mountains<|im_end|>',\n",
       " '<|im_start|>olympus<|im_end|>',\n",
       " '<|im_start|>fuji<|im_end|>',\n",
       " '<|im_start|>ben wyvis<|im_end|>',\n",
       " '<|im_start|>logan, foraker, elbrus<|im_end|>',\n",
       " '<|im_start|>kazbek, matterhorn, distaghil sar<|im_end|>',\n",
       " '<|im_start|>pyramid peak, kosciuszko, lhotse<|im_end|>',\n",
       " '<|im_start|>robson, jabal al-madbah, cook<|im_end|>',\n",
       " '<|im_start|>elgon, nanga parbat, ojos del salado<|im_end|>',\n",
       " '<|im_start|>vesuvius, aconcagua, sanford<|im_end|>',\n",
       " '<|im_start|>whistler, olympus, hunter<|im_end|>',\n",
       " '<|im_start|>broad peak, ben nevis, everest<|im_end|>',\n",
       " '<|im_start|>ben nevis, aneto, kilimanjaro<|im_end|>',\n",
       " '<|im_start|>machu picchu, apo, vinson<|im_end|>',\n",
       " '<|im_start|>everest, kilimanjaro, cook<|im_end|>',\n",
       " '<|im_start|>vinson massif, elbrus, kenya<|im_end|>',\n",
       " '<|im_start|>washington, lucania, fitz roy<|im_end|>',\n",
       " '<|im_start|>denali, leconte, aconcagua, huascarã¡n, chimborazo<|im_end|>',\n",
       " '<|im_start|>bona, mont blanc, table mountain<|im_end|>',\n",
       " '<|im_start|>banff national park, everest, grand teton<|im_end|>',\n",
       " '<|im_start|>matterhorn, damavand, temple<|im_end|>',\n",
       " '<|im_start|>manaslu, kilimanjaro, kirkjufell<|im_end|>',\n",
       " '<|im_start|>wrangell, tibesti, fuji<|im_end|>',\n",
       " '<|im_start|>wilhelm, ben nevis, cho oyu<|im_end|>',\n",
       " '<|im_start|>huascarã¡n, elbert, annapurna<|im_end|>',\n",
       " '<|im_start|>store skagastã¸lstind, matterhorn, blackburn<|im_end|>',\n",
       " '<|im_start|>mckinley, cook, huascaran<|im_end|>',\n",
       " '<|im_start|>crillon, denali, mckinley, logan<|im_end|>',\n",
       " '<|im_start|>ararat, denali, gunnbjã¸rn fjeld<|im_end|>',\n",
       " '<|im_start|>ararat, rainier, gasherbrum i<|im_end|>',\n",
       " '<|im_start|>yosemite, augustus, jungfrau<|im_end|>',\n",
       " '<|im_start|>mckinley, elbrus, washington<|im_end|>',\n",
       " '<|im_start|>elbrus, matanga hill, whakaari<|im_end|>',\n",
       " '<|im_start|>gyachung kang, mont blanc, andes<|im_end|>',\n",
       " '<|im_start|>mauna kea, elbrus, kilimanjaro<|im_end|>',\n",
       " '<|im_start|>hood, everest, kailash<|im_end|>',\n",
       " '<|im_start|>pulag, rose, ararat<|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|><|im_end|>',\n",
       " '<|im_start|>cerro torre, cerro torre, cerro torre<|im_end|>',\n",
       " '<|im_start|>stanley, stanley, stanley<|im_end|>',\n",
       " '<|im_start|>elbrus, elbrus, elbrus<|im_end|>',\n",
       " '<|im_start|>karl marx peak<|im_end|>',\n",
       " '<|im_start|>belukha mountain<|im_end|>',\n",
       " '<|im_start|>wilhelm<|im_end|>',\n",
       " '<|im_start|>kazbek<|im_end|>',\n",
       " '<|im_start|>cerro torre<|im_end|>',\n",
       " '<|im_start|>monte san lorenzo<|im_end|>',\n",
       " '<|im_start|>noshaq<|im_end|>',\n",
       " '<|im_start|>puncak mandala<|im_end|>',\n",
       " '<|im_start|>nevado huascarã¡n, nevado huascarã¡n<|im_end|>',\n",
       " '<|im_start|>huascarã¡n, huascarã¡n<|im_end|>',\n",
       " '<|im_start|>elbrus, elbrus<|im_end|>',\n",
       " '<|im_start|>yari, yari, yari<|im_end|>',\n",
       " '<|im_start|>kangchenjunga, kangchenjunga<|im_end|>',\n",
       " '<|im_start|>huascarã¡n, huascarã¡n<|im_end|>',\n",
       " '<|im_start|>kenya, kenya, kirinyaga<|im_end|>',\n",
       " '<|im_start|>cerro torre, cerro torre<|im_end|>',\n",
       " '<|im_start|>ama dablam, ama dablam, ama dablam<|im_end|>',\n",
       " '<|im_start|>kazbek, kazbek, kazbek<|im_end|>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eace7ab3-072d-4453-864f-fa6847d0e926",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16966, 296, 377, 258, 3258]\n",
      "[100264, 76, 377, 258, 3258, 100265]\n"
     ]
    }
   ],
   "source": [
    "print(enc_Y[0])\n",
    "print(enc_Y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "823db793-9afe-4ac7-9a81-7ff7d7473d01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56950, 90256, 30633, 35687, 48501, 11822, 42211, 33889, 3221, 6606, 296, 377, 258, 3258, 13]\n",
      "[100264, 56950, 90256, 30633, 35687, 48501, 11822, 42211, 33889, 3221, 6606, 296, 377, 258, 3258, 13, 100265]\n"
     ]
    }
   ],
   "source": [
    "print(enc_X[0])\n",
    "print(enc_X2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1979bbe-62d3-4b58-a1d8-09694b0c17ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lists(lst, max_len):\n",
    "    if len(lst) < max_len:\n",
    "        lst.extend([0] * (max_len - len(lst)))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbbc1590-14fb-4be4-8bb0-c99740da6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = [adjust_lists(lst, 105) for lst in enc_X2] \n",
    "flabels = [adjust_lists(lst, 106) for lst in enc_Y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4341fa9f-71c9-428c-80ba-faff6f6dfbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_X = [len(l) for l in enc_X]\n",
    "lens_Y = [len(l) for l in enc_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9593467d-e92f-403b-84f0-e13954fc8beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(max(lens_X))\n",
    "print(max(lens_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20141ac6-2f9f-4fc6-8673-49b4dd96b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fdata, \n",
    "                                                    flabels, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02a639-7d04-4c79-90f6-b4ffbd2ceed1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af96b688-75f9-4b5c-a9a3-7a1f216c580f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=469):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]`` no\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db66b5ba-0895-4bcd-bf2e-a36e9501ba13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab, maxlen=150, embedding_dim=16, dropout_rate=0.1):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.emb = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.pos_emb = PositionalEncoding(embedding_dim, dropout=dropout_rate, max_len=maxlen)\n",
    "    def forward(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        x = x * math.sqrt(self.embedding_dim) #!!!! \n",
    "        x = self.pos_emb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d3b44f-2afe-464f-a608-414d5d65dd8c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, fully_connected_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(fully_connected_dim, embedding_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2c2892e-f027-434e-8f4b-7779df1af35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "\n",
    "        self_mha_output, _ = self.mha(inputs, inputs, inputs, key_padding_mask=mask) # !!! mask\n",
    "\n",
    "        skip_attention = self.norm1(inputs + self_mha_output)\n",
    "\n",
    "        ffn_output = self.ffn(skip_attention)\n",
    "\n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "\n",
    "        encoder_layer_out = self.norm2(skip_attention + ffn_output)\n",
    "\n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "730e6ee5-1b92-4ce6-9076-353c89d65ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, fully_connected_dim, embedding_dim, max_len,\n",
    "                 num_vocab, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.fully_connected_dim = fully_connected_dim\n",
    "\n",
    "        self.pos_encoding = TokenEmbedding(num_vocab, max_len, embedding_dim, dropout_rate)\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(embedding_dim=embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)]\n",
    "                                        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.pos_encoding(inputs)\n",
    "        src_padding_mask = (inputs == 0)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, src_padding_mask) #mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e6777ad-175e-4c59-9640-c7627fbe1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = FullyConnected(embedding_dim, fully_connected_dim)\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.norm3 = nn.LayerNorm(normalized_shape=embedding_dim, eps=layernorm_eps)\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, inputs, enc_output, src_padding_mask): # look_ahead_mask !!!!\n",
    "        \n",
    "        seq_len = inputs.size(1)\n",
    "        ahead_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).to(device)\n",
    "        #ahead_mask = self.create_look_ahead_mask2(seq_len)\n",
    "        \n",
    "        self_mha1_output, _ = self.mha1(inputs, inputs, inputs, attn_mask=ahead_mask, key_padding_mask=src_padding_mask)#, key_padding_mask=src_padding_mask) # look_ahead_mask !!!! batch (pad)\n",
    "        Q1 = self.norm1(self_mha1_output + inputs)\n",
    "        \n",
    "        self_mha2_output, _ = self.mha2(query=Q1, key=enc_output, value=enc_output)#, key_padding_mask=src_padding_mask) # pad mask  ???\n",
    "        skip_attention2 = self.norm2(self_mha2_output + Q1)\n",
    "        \n",
    "        ffn_output = self.ffn(skip_attention2)\n",
    "        drop_output = self.dropout_ffn(ffn_output)\n",
    "        skip3 = self.norm3(drop_output + skip_attention2)\n",
    "        \n",
    "        return skip3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a09209fd-004d-406a-a1b1-3997391a8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim,  #target_vocab_size, maximum_position_encoding,\n",
    "                 num_vocab=35, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = TokenEmbedding(num_vocab=num_vocab, maxlen=maxlen, embedding_dim=embedding_dim) # num_vocab=34, maxlen=400, embedding_dim=64\n",
    "        \n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs, enc_output):\n",
    "        \n",
    "        src_padding_mask = (inputs == 0) \n",
    "        x = self.token_emb(inputs) # torch.Size([Batch, 400, 64])\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, src_padding_mask) # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b07760f2-1a20-46df-8af6-23a9e2f316a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers_encoder, num_layers_decoder, num_heads_encoder, num_heads_decoder,\n",
    "                 fully_connected_dim_encoder, fully_connected_dim_decoder, embedding_dim_encoder,\n",
    "                 embedding_dim_decoder, max_len_enc, max_len_dec, num_vocab, # мб same last 2\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers=num_layers_encoder,\n",
    "                               num_heads=num_heads_encoder,\n",
    "                               fully_connected_dim=fully_connected_dim_encoder,\n",
    "                               embedding_dim=embedding_dim_encoder,\n",
    "                               max_len=max_len_enc, # 116\n",
    "                               num_vocab=num_vocab,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps) # torch.Size([1, 400, 64])\n",
    "        \"\"\"num_class, num_layers, num_heads, fully_connected_dim, embedding_dim, max_len,\n",
    "                 num_vocab, dropout_rate=0.1, layernorm_eps=1e-6)\"\"\"\n",
    "        \n",
    "        self.decoder = Decoder(num_layers=num_layers_decoder, \n",
    "                               embedding_dim=embedding_dim_decoder,\n",
    "                               num_heads=num_heads_decoder,\n",
    "                               fully_connected_dim=fully_connected_dim_decoder,\n",
    "                               num_vocab=num_vocab, # num_vocab=35\n",
    "                               maxlen=max_len_dec,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "        \"\"\"num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 num_vocab=34, maxlen=400, dropout_rate=0.1, layernorm_eps=1e-6\"\"\"\n",
    "        \n",
    "        self.linear = nn.Linear(embedding_dim_decoder, num_vocab)\n",
    "        \n",
    "    def forward(self, enc_input, output_vect_str):\n",
    "        \n",
    "        enc_output = self.encoder(enc_input)\n",
    "        \n",
    "        dec_output = self.decoder(output_vect_str, enc_output)  # torch.Size([Batch, 400, 64])\n",
    "        \n",
    "        final_output = self.linear(dec_output)\n",
    "        \n",
    "        return final_output  # [Batch, 400, 35]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35301a7-db94-4938-a665-795de11f982c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Func Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e70c20e1-d564-47df-82f1-488a2699ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61bdfd50-4ab9-459a-a15d-f006aa48bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, opt, loader):\n",
    "    loss_per_batches = 0\n",
    "    elapsed = 0\n",
    "    start_epoch2 = time.time()\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        start_epoch = time.time()\n",
    "        features, labels = data\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        dec_input = labels[:, :-1]\n",
    "        dec_target = labels[:, 1:]\n",
    "        \n",
    "        y_pred = model(features, dec_input)\n",
    "        \n",
    "        #one_hot = nn.functional.one_hot(labels, 35).type(torch.float)\n",
    "        #indices = torch.nonzero(torch.eq(labels, 0))[0].item()\n",
    "        #print(str(labels) + \"y_pred\")\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        loss = loss_fn(y_pred.view(-1, y_pred.size(-1)), dec_target.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        loss_per_batches += loss\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        elapsed += (end_epoch - start_epoch)\n",
    "        \n",
    "    print(\"train = \" + str(elapsed))\n",
    "    print(\"train + load = \" + str(time.time() - start_epoch2))\n",
    "    return loss_per_batches/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23416871-a4d8-4aee-8d7d-16f05a5fd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, opt, train_loader, val_loader, save_treshold=5, epochs=10, model_name='model_name'):\n",
    "        \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/' + model_name + '_{}'.format(timestamp))\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_epoch = time.time()\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = train_step(model, loss_fn, opt, train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        vloss = 0\n",
    "        counter = 0\n",
    "        with torch.inference_mode():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vfeatures, vlabels = vdata\n",
    "                vfeatures, vlabels = vfeatures.to(device), vlabels.to(device)\n",
    "                dec_input = vlabels[:, :-1]\n",
    "                dec_target = vlabels[:, 1:]\n",
    "\n",
    "                y_pred = model(vfeatures, dec_input)\n",
    "\n",
    "                vloss += loss_fn(y_pred.view(-1, y_pred.size(-1)), dec_target.contiguous().view(-1))\n",
    "                counter = i\n",
    "\n",
    "        avg_vloss = vloss / (counter + 1)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        \n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % save_treshold == 0:\n",
    "            model_path = model_name +'_{}_{}'.format(timestamp, epoch)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        end_epoch = time.time()\n",
    "        elapsed = end_epoch - start_epoch\n",
    "        print(\"Time per epoch {}s\".format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fecc1-74bb-4e9b-bbdf-8ab62bdab66f",
   "metadata": {},
   "source": [
    "# DataLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eb4a3ea-1219-4c17-9ef3-ec3c8e11a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(X_train, y_train)\n",
    "vdataset = CustomDataset(X_test, y_test)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=12, num_workers=2, shuffle=True) # + num thread num_workers=6,\n",
    "vdataloader = torch.utils.data.DataLoader(vdataset, batch_size=12, num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb4bed-a001-4151-bacb-a9dabc6a0888",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddc1a2da-19f3-4ed6-b57a-335c27d5f0a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (pos_encoding): TokenEmbedding(\n",
       "      (emb): Embedding(100277, 64)\n",
       "      (pos_emb): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (token_emb): TokenEmbedding(\n",
       "      (emb): Embedding(100277, 64)\n",
       "      (pos_emb): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dec_layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (mha1): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (mha2): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout_ffn): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=100277, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(6, 3, 16, 8, \n",
    "                1024, 1024, 64,\n",
    "                64, 105, 105, enc2.n_vocab,\n",
    "                dropout_rate=0.25)\n",
    "\n",
    "\"\"\"num_layers_encoder, num_layers_decoder, num_heads_encoder, num_heads_decoder,\n",
    "                 fully_connected_dim_encoder, fully_connected_dim_decoder, embedding_dim_encoder,\n",
    "                 embedding_dim_decoder, max_len_enc, max_len_dec, num_vocab,\"\"\"\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "340570ff-35f6-4f2e-b08c-a54ff756f54d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "train = 3.7760729789733887\n",
      "train + load = 8.15526008605957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\da4nik\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 10.457783699035645 valid 9.595171928405762\n",
      "Time per epoch 11.038755893707275s\n",
      "EPOCH 2:\n",
      "train = 1.803419828414917\n",
      "train + load = 5.984614849090576\n",
      "LOSS train 8.828719139099121 valid 7.853949069976807\n",
      "Time per epoch 8.743976354598999s\n",
      "EPOCH 3:\n",
      "train = 1.8866515159606934\n",
      "train + load = 6.078395128250122\n",
      "LOSS train 7.069869041442871 valid 6.61879301071167\n",
      "Time per epoch 8.816003322601318s\n",
      "EPOCH 4:\n",
      "train = 1.8084006309509277\n",
      "train + load = 5.917388200759888\n",
      "LOSS train 5.77244234085083 valid 5.617888450622559\n",
      "Time per epoch 8.640006065368652s\n",
      "EPOCH 5:\n",
      "train = 1.9077973365783691\n",
      "train + load = 6.318501949310303\n",
      "LOSS train 5.062497615814209 valid 5.488475322723389\n",
      "Time per epoch 9.50612187385559s\n",
      "EPOCH 6:\n",
      "train = 1.939434289932251\n",
      "train + load = 7.367485046386719\n",
      "LOSS train 4.681278705596924 valid 4.748739719390869\n",
      "Time per epoch 10.268407344818115s\n",
      "EPOCH 7:\n",
      "train = 1.8643231391906738\n",
      "train + load = 6.285733699798584\n",
      "LOSS train 4.413488864898682 valid 4.870190620422363\n",
      "Time per epoch 9.082167387008667s\n",
      "EPOCH 8:\n",
      "train = 1.8364086151123047\n",
      "train + load = 6.045998573303223\n",
      "LOSS train 4.163295269012451 valid 4.735394477844238\n",
      "Time per epoch 8.833228349685669s\n",
      "EPOCH 9:\n",
      "train = 1.7967090606689453\n",
      "train + load = 6.097203969955444\n",
      "LOSS train 3.916381359100342 valid 4.008508205413818\n",
      "Time per epoch 8.781813383102417s\n",
      "EPOCH 10:\n",
      "train = 1.7563974857330322\n",
      "train + load = 5.949620723724365\n",
      "LOSS train 3.739642381668091 valid 3.8758952617645264\n",
      "Time per epoch 8.700937509536743s\n",
      "EPOCH 11:\n",
      "train = 1.854417324066162\n",
      "train + load = 6.077740669250488\n",
      "LOSS train 3.5018563270568848 valid 4.166920185089111\n",
      "Time per epoch 8.801441192626953s\n",
      "EPOCH 12:\n",
      "train = 1.7849042415618896\n",
      "train + load = 5.940985202789307\n",
      "LOSS train 3.3964762687683105 valid 3.963991165161133\n",
      "Time per epoch 8.726346731185913s\n",
      "EPOCH 13:\n",
      "train = 1.834409236907959\n",
      "train + load = 6.055510520935059\n",
      "LOSS train 3.170724868774414 valid 3.6171188354492188\n",
      "Time per epoch 8.777041673660278s\n",
      "EPOCH 14:\n",
      "train = 1.8444180488586426\n",
      "train + load = 6.188131093978882\n",
      "LOSS train 3.0411577224731445 valid 4.035253047943115\n",
      "Time per epoch 9.028948545455933s\n",
      "EPOCH 15:\n",
      "train = 1.92608642578125\n",
      "train + load = 6.2003960609436035\n",
      "LOSS train 2.907763719558716 valid 3.9996306896209717\n",
      "Time per epoch 9.057319164276123s\n",
      "EPOCH 16:\n",
      "train = 1.7684011459350586\n",
      "train + load = 5.965898513793945\n",
      "LOSS train 2.7461705207824707 valid 3.9550015926361084\n",
      "Time per epoch 8.727686882019043s\n",
      "EPOCH 17:\n",
      "train = 1.8644659519195557\n",
      "train + load = 6.0599446296691895\n",
      "LOSS train 2.6361000537872314 valid 3.870999574661255\n",
      "Time per epoch 8.814560890197754s\n",
      "EPOCH 18:\n",
      "train = 1.7593982219696045\n",
      "train + load = 5.906870365142822\n",
      "LOSS train 2.4805426597595215 valid 3.60380220413208\n",
      "Time per epoch 8.691975355148315s\n",
      "EPOCH 19:\n",
      "train = 1.8355185985565186\n",
      "train + load = 6.050480842590332\n",
      "LOSS train 2.468205451965332 valid 3.813938856124878\n",
      "Time per epoch 8.798690557479858s\n",
      "EPOCH 20:\n",
      "train = 1.7991716861724854\n",
      "train + load = 5.960705041885376\n",
      "LOSS train 2.3479135036468506 valid 4.088016033172607\n",
      "Time per epoch 8.698760271072388s\n",
      "EPOCH 21:\n",
      "train = 1.8784263134002686\n",
      "train + load = 6.06225061416626\n",
      "LOSS train 2.2725601196289062 valid 3.6549041271209717\n",
      "Time per epoch 8.835870265960693s\n",
      "EPOCH 22:\n",
      "train = 1.827439546585083\n",
      "train + load = 6.077138662338257\n",
      "LOSS train 2.1855828762054443 valid 3.894385576248169\n",
      "Time per epoch 8.841762781143188s\n",
      "EPOCH 23:\n",
      "train = 1.9164316654205322\n",
      "train + load = 6.146552085876465\n",
      "LOSS train 2.139478921890259 valid 3.5524539947509766\n",
      "Time per epoch 9.085222959518433s\n",
      "EPOCH 24:\n",
      "train = 1.9514429569244385\n",
      "train + load = 6.318284511566162\n",
      "LOSS train 2.074913263320923 valid 3.892813205718994\n",
      "Time per epoch 9.190934896469116s\n",
      "EPOCH 25:\n",
      "train = 1.8954222202301025\n",
      "train + load = 6.1860671043396\n",
      "LOSS train 1.9728820323944092 valid 3.391883134841919\n",
      "Time per epoch 9.069256067276001s\n",
      "EPOCH 26:\n",
      "train = 1.813424825668335\n",
      "train + load = 6.4219279289245605\n",
      "LOSS train 1.9504203796386719 valid 4.889214038848877\n",
      "Time per epoch 9.23858642578125s\n",
      "EPOCH 27:\n",
      "train = 1.7223913669586182\n",
      "train + load = 6.125163793563843\n",
      "LOSS train 1.881245732307434 valid 3.944887638092041\n",
      "Time per epoch 9.03881025314331s\n",
      "EPOCH 28:\n",
      "train = 1.9294369220733643\n",
      "train + load = 6.361894369125366\n",
      "LOSS train 1.82091224193573 valid 3.2525410652160645\n",
      "Time per epoch 9.311529159545898s\n",
      "EPOCH 29:\n",
      "train = 1.8624351024627686\n",
      "train + load = 5.985490322113037\n",
      "LOSS train 1.7914931774139404 valid 3.4648170471191406\n",
      "Time per epoch 8.78146481513977s\n",
      "EPOCH 30:\n",
      "train = 1.7364397048950195\n",
      "train + load = 5.853571176528931\n",
      "LOSS train 1.7135478258132935 valid 4.254312515258789\n",
      "Time per epoch 8.707407474517822s\n",
      "EPOCH 31:\n",
      "train = 1.7374012470245361\n",
      "train + load = 5.993093013763428\n",
      "LOSS train 1.6834145784378052 valid 3.499427080154419\n",
      "Time per epoch 8.855753898620605s\n",
      "EPOCH 32:\n",
      "train = 1.7353966236114502\n",
      "train + load = 6.281903505325317\n",
      "LOSS train 1.6117802858352661 valid 3.547212600708008\n",
      "Time per epoch 9.21716856956482s\n",
      "EPOCH 33:\n",
      "train = 1.8128819465637207\n",
      "train + load = 6.054623365402222\n",
      "LOSS train 1.5814422369003296 valid 3.511228322982788\n",
      "Time per epoch 8.86828064918518s\n",
      "EPOCH 34:\n",
      "train = 1.8213708400726318\n",
      "train + load = 6.00409460067749\n",
      "LOSS train 1.5188418626785278 valid 3.5555481910705566\n",
      "Time per epoch 8.7107834815979s\n",
      "EPOCH 35:\n",
      "train = 1.7133831977844238\n",
      "train + load = 5.920548677444458\n",
      "LOSS train 1.4918800592422485 valid 4.583334445953369\n",
      "Time per epoch 8.728174448013306s\n",
      "EPOCH 36:\n",
      "train = 1.7683823108673096\n",
      "train + load = 5.852055311203003\n",
      "LOSS train 1.4646837711334229 valid 3.755417585372925\n",
      "Time per epoch 8.567916870117188s\n",
      "EPOCH 37:\n",
      "train = 1.835409164428711\n",
      "train + load = 5.905006170272827\n",
      "LOSS train 1.4577627182006836 valid 4.070311546325684\n",
      "Time per epoch 8.56805157661438s\n",
      "EPOCH 38:\n",
      "train = 1.7753970623016357\n",
      "train + load = 6.011741876602173\n",
      "LOSS train 1.3816660642623901 valid 3.74796724319458\n",
      "Time per epoch 8.87180471420288s\n",
      "EPOCH 39:\n",
      "train = 1.6883752346038818\n",
      "train + load = 5.7942399978637695\n",
      "LOSS train 1.3701817989349365 valid 3.653385639190674\n",
      "Time per epoch 8.497132301330566s\n",
      "EPOCH 40:\n",
      "train = 1.8398780822753906\n",
      "train + load = 5.89863133430481\n",
      "LOSS train 1.345655083656311 valid 5.2872209548950195\n",
      "Time per epoch 8.503334045410156s\n",
      "EPOCH 41:\n",
      "train = 1.7874021530151367\n",
      "train + load = 5.858894348144531\n",
      "LOSS train 1.3311434984207153 valid 3.7777230739593506\n",
      "Time per epoch 8.475264072418213s\n",
      "EPOCH 42:\n",
      "train = 1.847869873046875\n",
      "train + load = 6.036227703094482\n",
      "LOSS train 1.303673267364502 valid 4.239441871643066\n",
      "Time per epoch 8.916300773620605s\n",
      "EPOCH 43:\n",
      "train = 1.8414149284362793\n",
      "train + load = 6.361574172973633\n",
      "LOSS train 1.2969547510147095 valid 4.229949951171875\n",
      "Time per epoch 9.097068309783936s\n",
      "EPOCH 44:\n",
      "train = 1.745396614074707\n",
      "train + load = 5.930354356765747\n",
      "LOSS train 1.2545771598815918 valid 5.810323238372803\n",
      "Time per epoch 8.691504955291748s\n",
      "EPOCH 45:\n",
      "train = 1.7853991985321045\n",
      "train + load = 5.870584487915039\n",
      "LOSS train 1.2649561166763306 valid 4.73513126373291\n",
      "Time per epoch 8.666208744049072s\n",
      "EPOCH 46:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst_try\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, opt, train_loader, val_loader, save_treshold, epochs, model_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 12\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     15\u001b[0m vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, loss_fn, opt, loader)\u001b[0m\n\u001b[0;32m      3\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m start_epoch2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m      7\u001b[0m     start_epoch \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m     features, labels \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1317\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[1;32m-> 1317\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \n\u001b[0;32m   1322\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1442\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[0;32m   1444\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:109\u001b[0m, in \u001b[0;36mPopen.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     msecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m--> 109\u001b[0m res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForSingleObject(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle), msecs)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWAIT_OBJECT_0:\n\u001b[0;32m    111\u001b[0m     code \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mGetExitCodeProcess(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, dataloader, vdataloader , 50, epochs=100, model_name='first_try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0daf8-84a2-4499-892a-43648b2c86ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
